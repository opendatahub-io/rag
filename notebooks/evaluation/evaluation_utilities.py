# Copyright 2025 IBM, Red Hat
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import string
from ragas.evaluation import evaluate
from ragas import EvaluationDataset

from llama_index.core.node_parser import MarkdownNodeParser
from llama_index.readers.docling import DoclingReader
from llama_index.core import VectorStoreIndex, get_response_synthesizer
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.llms import ChatMessage
from llama_index.core.postprocessor import LLMRerank
from llama_index.core.schema import QueryBundle
from llama_index.core.llms.llm import LLM

from scipy.stats import permutation_test

import numpy as np
import pandas as pd
import itertools

import os
import json
import re
import math
import logging


from pathlib import Path
import copy
import time


LOGGER = logging.getLogger(__name__)

# Generated by Google Gemini
if not LOGGER.handlers:
    # Create a new StreamHandler (to dump output to the console/stderr)
    console_handler = logging.StreamHandler()

    # Set the level for this handler to DEBUG
    # This means this handler will process and output messages of DEBUG level and higher.
    console_handler.setLevel(logging.DEBUG)

    # (Optional but Recommended) Define a formatter for clear output
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    console_handler.setFormatter(formatter)

    # Add the handler to your specific LOGGER
    LOGGER.addHandler(console_handler)


# This is the default RAG prompt template for Llama Index
PROMPT_TEMPLATE = "Context information is below.\n---------------------\n{context_str}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: {query_str}\nAnswer: "


def make_simple_index(file_paths, embed_model):
    # Doc string assisted by watsonx Code Assistant
    """
    Create a simple Llama Index vector store index from a list of file paths using a specified embedding model.

    This function takes a list of file paths and an embedding model as input. It uses the DoclingReader
    to load data from the provided file paths and the MarkdownNodeParser to parse the documents.
    The VectorStoreIndex is then created using the loaded documents, transformations, and the specified
    embedding model.

    Args:
        file_paths (List[str]): A list of file paths from which to load documents.
        embed_model (EmbedModel): The embedding model to use for vectorization.

    Returns:
        VectorStoreIndex: A vector store index created from the provided file paths and embedding model.
    """
    reader = DoclingReader()
    node_parser = MarkdownNodeParser()

    index = VectorStoreIndex.from_documents(
        documents=reader.load_data(file_path=file_paths),
        transformations=[node_parser],
        embed_model=embed_model,
    )
    return index


# Generated by Cursor with claude-4-sonnet
def run_rag_with_progress(
    qna, output_file, single_entry_processor_func, **processor_kwargs
):
    """
    Generic function to run any RAG process with automatic progress saving and crash recovery.

    This function handles the common pattern of:
    1. Loading existing progress
    2. Processing entries one by one using a provided function
    3. Saving progress after each entry
    4. Saving final results and cleanup

    Args:
        qna (list): A list of dictionaries, where each dictionary contains 'user_input' (question)
        output_file (str): Path to save the final results and progress tracking
        single_entry_processor_func (callable): Function to process a single entry
        **processor_kwargs: Additional keyword arguments to pass to the processor function

    Returns:
        list: The processed dataset
    """
    # Load any existing progress
    progress_file = output_file + ".progress"
    completed_entries, start_index = load_rag_progress(progress_file)

    if completed_entries:
        LOGGER.info(
            f"Resuming from entry {start_index} (found {len(completed_entries)} completed entries)"
        )
    else:
        LOGGER.info("Starting fresh")

    output_dataset = [None] * start_index

    # Apply completed entries to the dataset
    for index, entry in completed_entries.items():
        if index < start_index:
            output_dataset[index] = entry

    # Check for any None values (gaps) in completed entries that need to be processed
    missing_indices = [i for i in range(start_index) if output_dataset[i] is None]
    remaining_indices = list(range(start_index, len(qna)))

    # Combine missing indices with remaining indices to get all indices that need processing
    indices_to_process = missing_indices + remaining_indices

    if missing_indices:
        LOGGER.warning(
            f"Found {len(missing_indices)} gaps in completed entries that will be reprocessed: {missing_indices}"
        )

    # Process all needed entries
    for i in indices_to_process:
        LOGGER.info(f"Processing entry {i + 1}/{len(qna)}")

        # Process this single entry using the provided processor function
        single_entry_result = single_entry_processor_func(qna[i], **processor_kwargs)

        # Update the dataset with the result - handle both gaps and new entries
        if i < len(output_dataset):
            output_dataset[i] = single_entry_result
        else:
            output_dataset.append(single_entry_result)

        # Save progress immediately
        save_rag_progress(single_entry_result, progress_file, i)

    # Save final results
    write_json(output_dataset, output_file)

    # Clean up progress file
    if os.path.exists(progress_file):
        os.remove(progress_file)

    LOGGER.info(f"Completed processing all {len(output_dataset)} entries")
    return output_dataset


# Assisted by Cursor with claude-4-sonnet
# For details on the APIs used, see https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/usage_pattern/#low-level-composition-api
def run_rag(qna, generator_model, idx, output_file, number_of_search_results=5):
    # Doc string assisted by watsonx Code Assistant
    """
    Run the Retrieval-Augmented Generation (RAG) process on a given dataset using LlamaIndex.

    This function takes a dataset of questions and answers, a language model for response generation,
    an index for retrieval, and an optional parameter for the number of search results.
    It returns the dataset with added responses generated by the language model and the retrieved contexts.

    This function includes automatic progress saving and crash recovery. It will save progress to a file
    and resume from where it left off if the process is interrupted.

    Parameters:
    qna (list): A list of dictionaries, where each dictionary contains 'user_input' (question).
    generator_model (LLM): A pre-trained language model for generating responses.
    idx (Index): An index for efficient retrieval of relevant contexts.
    output_file (str): Path to save the final results and progress tracking.
    number_of_search_results (int, optional): The number of top contexts to retrieve for each question. Defaults to 5.

    Returns:
    list: A modified copy of the dataset with added 'response' (answer) and 'retrieved_contexts' (search results) fields.
    """
    return run_rag_with_progress(
        qna,
        output_file,
        run_rag_single_entry,
        generator_model=generator_model,
        idx=idx,
        number_of_search_results=number_of_search_results,
    )


def run_rag_single_entry(entry, generator_model, idx, number_of_search_results=5):
    """
    Core RAG logic for processing a single Q&A entry.

    This function contains the main RAG processing logic separated from progress tracking.

    Args:
        entry (dict): A single Q&A entry containing 'user_input' (question)
        generator_model (LLM): A pre-trained language model for generating responses
        idx (Index): An index for efficient retrieval of relevant contexts
        number_of_search_results (int): The number of top contexts to retrieve

    Returns:
        dict: The entry with added "response" and "retrieved_contexts" fields
    """
    # Make a copy to avoid modifying the original
    result_entry = copy.deepcopy(entry)

    retriever = VectorIndexRetriever(
        index=idx,
        similarity_top_k=number_of_search_results,
    )

    query_engine = RetrieverQueryEngine(
        retriever=retriever,
        response_synthesizer=get_response_synthesizer(llm=generator_model),
    )

    question = result_entry["user_input"]
    result = query_engine.query(question)
    result_entry["response"] = str(result).strip()
    result_entry["retrieved_contexts"] = [n.text for n in result.source_nodes]

    return result_entry


# Generated by Cursor with claude-4-sonnet
def save_rag_progress(entry, output_file, index):
    """
    Save a single completed RAG entry to disk for crash recovery.

    Args:
        entry (dict): The completed Q&A entry with reference and reference_contexts
        output_file (str): Path to the progress file
        index (int): The index of this entry in the dataset
    """
    progress_entry = {"index": index, "entry": entry, "timestamp": time.time()}

    # Append to JSON Lines file for incremental writing
    with open(output_file, "a") as f:
        f.write(json.dumps(progress_entry) + "\n")


# Generated by Cursor with claude-4-sonnet
def load_rag_progress(output_file):
    """
    Load existing progress from disk and determine where to resume.

    Args:
        output_file (str): Path to the progress file

    Returns:
        tuple: (completed_entries_dict, next_start_index)
            - completed_entries_dict: dict mapping index -> completed entry
            - next_start_index: index to start processing from
    """
    completed_entries = {}
    max_completed_index = -1

    if os.path.exists(output_file):
        try:
            with open(output_file, "r") as f:
                for line in f:
                    line = line.strip()
                    if line:
                        progress_entry = json.loads(line)
                        index = progress_entry["index"]
                        entry = progress_entry["entry"]
                        completed_entries[index] = entry
                        max_completed_index = max(max_completed_index, index)
        except (json.JSONDecodeError, KeyError, IOError) as e:
            LOGGER.warning(f"Could not load progress file {output_file}: {e}")
            LOGGER.warning("Starting from beginning...")

    next_start_index = max_completed_index + 1
    return completed_entries, next_start_index


# Assisted by Cursor with claude-4-sonnet
def run_reference_rag(
    qna,
    generator_model,
    idx,
    output_file,
    number_of_search_results=20,
    number_of_selected_results=5,
):
    # Doc string assisted by watsonx Code Assistant
    """
    This function runs a Retrieval-Augmented Generation (RAG) process on a given dataset using LlamaIndex.  It is designed to provide "reference"
    answers, i.e., answers that can be treated as a proxy for "ground truth" answers for evaluating other RAG solutions.  To be used that way,
    it is important to run this with the highest quality generator model and index that you can provide.  It retrieves more search results than
    it needs to generate answers then uses LlamaIndex's built-in LLMRerank to intelligently rank and select the most relevant passages.
    It then generates answers using those highest-ranked passages.  This would be too slow for most practical RAG applications, but it is useful
    for providing reference answers to compare other (presumably faster) RAG solutions to.

    This function also includes automatic progress saving and crash recovery.  It will save progress to a file and resume from where it left off
    if the process is interrupted.  It will also write the final results to a file.

    Parameters:
    qna (list): A list of dictionaries, where each dictionary contains 'user_input' (question).
    generator_model (LLM): A pre-trained language model for generating responses and reranking.
    idx (Index): An index for efficient retrieval of relevant contexts.
    output_file (str): Path to save the final results and progress tracking.
    number_of_search_results (int, optional): The number of top contexts to retrieve for each question. Defaults to 20.
    number_of_selected_results (int): The maximum number of top results to select as reference contexts for the generator model. Default is 5.

    Returns:
    list: The modified Q&A dataset with added "reference" and "reference_contexts" fields.  For each question, the "reference_contexts" will
    include up to number_of_selected_results entries, ranked by relevance using LLMRerank.
    """
    return run_rag_with_progress(
        qna,
        output_file,
        run_reference_rag_single_entry,
        generator_model=generator_model,
        idx=idx,
        number_of_search_results=number_of_search_results,
        number_of_selected_results=number_of_selected_results,
    )


def run_reference_rag_single_entry(
    entry,
    generator_model,
    idx,
    number_of_search_results=20,
    number_of_selected_results=5,
):
    """
    Core RAG logic for processing a single Q&A entry.

    This function contains the main RAG processing logic separated from progress tracking.

    Args:
        entry (dict): A single Q&A entry containing 'user_input' (question)
        generator_model (LLM): A pre-trained language model for generating responses and reranking
        idx (Index): An index for efficient retrieval of relevant contexts
        number_of_search_results (int): The number of top contexts to retrieve
        number_of_selected_results (int): The maximum number of top results to select as reference contexts

    Returns:
        dict: The entry with added "reference" and "reference_contexts" fields
    """
    # Make a copy to avoid modifying the original
    result_entry = copy.deepcopy(entry)

    retriever = VectorIndexRetriever(
        index=idx,
        similarity_top_k=number_of_search_results,
    )

    # Create LLMRerank postprocessor
    reranker = LLMRerank(
        llm=generator_model,
        top_n=number_of_selected_results,
        # Process nodes in batches for efficiency.  You don't want to make this the batch size too big because you'll run out of context window
        # size for the model or just confuse the model with more context than it can handle all at once.
        choice_batch_size=10,
    )

    question = result_entry["user_input"]

    # Retrieve initial set of nodes
    nodes = retriever.retrieve(question)

    if not nodes:
        result_entry["reference"] = ""
        result_entry["reference_contexts"] = []
        return result_entry

    # Use LLMRerank to rerank and select the best nodes
    query_bundle = QueryBundle(query_str=question)
    reranked_nodes = reranker._postprocess_nodes(nodes, query_bundle)

    # Note that this can happen if the question is not relevant to the index: the reranker will return
    # an empty list and then we have no reference contexts.  We could still ask the generator model to generate
    # an answer, but it will be a generic answer that is not useful for evaluation of RAG solutions.
    if not reranked_nodes:
        result_entry["reference"] = ""
        result_entry["reference_contexts"] = []
        return result_entry

    # Extract the text from the reranked nodes
    reference_contexts = [node.node.text for node in reranked_nodes]
    context_str = "\n-------\n".join(reference_contexts)

    # Generate the reference answer using the selected contexts
    message_text = PROMPT_TEMPLATE.format(
        **{"context_str": context_str, "query_str": question}
    )
    messages = [ChatMessage(role="user", content=message_text)]
    response = generator_model.chat(messages)

    result_entry["reference"] = response.message.blocks[0].text.strip()
    result_entry["reference_contexts"] = reference_contexts

    return result_entry


# Assisted by watsonx Code Assistant
def list_files(path):
    """
    Lists all files in the given directory path.

    Args:
    path: A single file or directory.

    Returns:
    list: If the input is a single file, a list containing only that file.  If the input is a directory, a list of all the files in that directory.
    """
    if os.path.isfile(path):
        return [Path(path)]
    files = []
    for file in os.listdir(Path(path)):
        file_path = Path(os.path.join(path, file))
        if os.path.isfile(file_path):
            files.append(file_path)
    return files


# Assisted by watsonx Code Assistant
def write_json(data, filename):
    """
    Writes the provided data to a JSON file.

    Parameters:
    data (dict or list): The data to be written to the file. It should be in a format that can be serialized to JSON.
    filename (str): The name of the file to write to.

    Returns:
    None
    """
    with open(filename, "w") as f:
        json.dump(data, f, indent=4)


# Assisted by watsonx Code Assistant
def read_json(filename):
    """
    Reads a JSON file and returns its content as a Python dictionary.

    Args:
    filename (str): The path to the JSON file to be read.

    Returns:
    dict: The content of the JSON file as a Python dictionary.

    Raises:
    FileNotFoundError: If the specified file does not exist.
    json.JSONDecodeError: If the file is not a valid JSON file.
    """
    with open(filename, "r") as f:
        data_loaded = json.load(f)
        return data_loaded


# Rubrics from https://github.com/instructlab/eval/blob/main/src/instructlab/eval/ragas.py which got them from ragas v0.2.11
# and has them "hardcoded in case ragas makes any changes to their DEFAULT_WITH_REFERENCE_RUBRICS in the future".
SCORING_RUBRICS = {
    "score1_description": "The response is entirely incorrect, irrelevant, or does not align with the reference in any meaningful way.",
    "score2_description": "The response partially matches the reference but contains major errors, significant omissions, or irrelevant information.",
    "score3_description": "The response aligns with the reference overall but lacks sufficient detail, clarity, or contains minor inaccuracies.",
    "score4_description": "The response is mostly accurate, aligns closely with the reference, and contains only minor issues or omissions.",
    "score5_description": "The response is fully accurate, completely aligns with the reference, and is clear, thorough, and detailed.",
}


def run_ragas(datasets, evaluator_llm_for_ragas, metrics):
    # Doc string assisted by watsonx Code Assistant
    """
    Evaluate answers to questions on given datasets using specified metrics and language model.

    This function takes a dictionary of datasets, a language model for evaluation, and a list of metrics.
    It evaluates each dataset using the provided language model and metrics, and returns a dictionary
    where the keys are the dataset labels and the values are the corresponding evaluation results.

    Parameters:
    datasets (dict): A dictionary where keys are dataset labels and values are lists of data points.
    evaluator_llm_for_ragas (Ragas LLM): A language model for Ragas to use to do evaluations.
    metrics (list): A list of evaluation metrics to use.

    Returns:
    dict: A dictionary where keys are dataset labels and values are the corresponding evaluation results.
    """
    results = {}
    for label, data in datasets.items():
        evaluation_dataset = EvaluationDataset.from_list(data)
        result = evaluate(
            metrics=metrics,
            batch_size=4,
            dataset=evaluation_dataset,
            llm=evaluator_llm_for_ragas,
            show_progress=True,
        )
        results[label] = result
    return results


def run_evaluation_of_questions_without_reference_answers(
    datasets_without_reference_answers, evaluator_llm_for_ragas
):
    # Doc string assisted by Cursor / claude-4-sonnet
    """
    Evaluate datasets by checking if responses incorrectly attempt to answer questions that should not be answered.

    This function evaluates RAG systems on questions where no reference answers are available, meaning
    the correct behavior is for the system to NOT attempt to answer (e.g., by saying "I don't know" or
    "This information is not available in the provided context"). This is crucial for RAG systems designed
    to only answer questions based on their corpus - they should refuse to answer rather than hallucinate
    responses for questions not covered by their knowledge base.

    The evaluation uses an LLM to determine whether each response is inappropriately attempting to answer
    questions that should be declined. This helps identify cases where the RAG system fails to properly
    recognize the limits of its knowledge and generates potentially incorrect or hallucinated answers.

    Parameters:
    datasets_without_reference_answers (dict): A dictionary where keys are dataset labels (strings) and
        values are lists of dictionaries. Each inner dictionary must contain 'user_input' (the question)
        and 'response' (the generated answer) fields. These should be questions where the correct behavior
        is to not provide an answer.
    evaluator_llm_for_ragas (LLM): A language model instance used to evaluate whether responses are
        inappropriately attempting to answer questions. This should be a capable model for making
        relevance judgments.

    Returns:
    dict: A dictionary where keys are the same dataset labels from the input and values are lists of
        integer scores. Each score corresponds to a question-answer pair in the dataset:
        - 0: The response IS attempting to answer the question (bad behavior - should have declined)
        - 1: The response is NOT attempting to answer the question (good behavior - correctly declined)
        Higher scores indicate better performance. The average score represents the percentage of
        questions where the system correctly refused to answer.

    Note:
    This evaluation is specifically for questions where the ground truth behavior is to NOT answer.
    It penalizes systems that attempt to answer questions they should decline.
    """
    results = {}
    for label, data in datasets_without_reference_answers.items():
        scores = []
        for qna in data:
            question = qna["user_input"]
            response = qna["response"]
            if check_if_answer_is_attempting_to_answer_question(
                question, response, evaluator_llm_for_ragas
            ):
                scores.append(0)
            else:
                scores.append(1)
        results[label] = pd.DataFrame(scores, columns=["Percent Unanswered"])

    return results


# Adapted from https://github.com/opendatahub-io/llama-stack-demos/blob/main/demos/rag_eval/Agentic_RAG_with_reference_eval.ipynb
def permutation_test_for_paired_samples(scores_a, scores_b, iterations=10_000):
    """
    Performs a permutation test of a given statistic on provided data.
    """

    def _statistic(x, y, axis):
        return np.mean(x, axis=axis) - np.mean(y, axis=axis)

    result = permutation_test(
        data=(scores_a, scores_b),
        statistic=_statistic,
        n_resamples=iterations,
        alternative="two-sided",
        permutation_type="samples",
    )
    return float(result.pvalue)


# Adapted from https://github.com/opendatahub-io/llama-stack-demos/blob/main/demos/rag_eval/Agentic_RAG_with_reference_eval.ipynb
def print_stats_significance(scores_a, scores_b, overview_label, label_a, label_b):
    """
    Runs permutation_test_for_paired_samples above, prints out the output, and returns true IFF there is a significant difference
    """

    mean_score_a = np.mean(scores_a)
    mean_score_b = np.mean(scores_b)

    p_value = permutation_test_for_paired_samples(scores_a, scores_b)
    print(overview_label)
    print(f" {label_a:<50}: {mean_score_a:>10.4f}")
    print(f" {label_b:<50}: {mean_score_b:>10.4f}")
    print(f" {'p_value':<50}: {p_value:>10.4f}")

    if p_value < 0.05:
        print("  p_value<0.05 so this result is statistically significant")
        # Note that the logic below if wrong if the mean scores are equal, but that can't be true if p<1.
        higher_model_id = label_a if mean_score_a >= mean_score_b else label_b
        print(
            f"  You can conclude that {higher_model_id} generation is better on data of this sort"
        )
        return True, p_value, mean_score_a, mean_score_b
    else:
        print("  p_value>=0.05 so this result is NOT statistically significant.")
        print(
            "  You can conclude that there is not enough data to tell which is better."
        )
        num_samples = len(scores_a)
        margin_of_error = 1 / math.sqrt(num_samples)
        print(
            f"  Note that this data includes {num_samples} questions which typically produces a margin of error of around +/-{margin_of_error:.1%}."
        )
        print("  So the two are probably roughly within that margin of error or so.")
        return False, p_value, mean_score_a, mean_score_b


def report_results_with_significance(results, metrics, subset_of_rows=None):
    """
    Iterates through all pairs of results for all metrics and computes the mean value of the metrics and whether the results are signifant.
    If subset_of_rows is set, then only that subset of all the rows are used.  (For example, this can be used to report results for just
    the subset of questions for which there is at least one reference context).
    """

    result_pairs = list(itertools.combinations(results.keys(), 2))
    result_summary = []
    for result_pair in result_pairs:
        result_summary_for_metric = {}
        for metric in metrics:
            results0 = (
                results[result_pair[0]]
                if isinstance(results[result_pair[0]], pd.DataFrame)
                else results[result_pair[0]].to_pandas()
            )
            results1 = (
                results[result_pair[1]]
                if isinstance(results[result_pair[1]], pd.DataFrame)
                else results[result_pair[1]].to_pandas()
            )

            if subset_of_rows:
                results0 = results0.iloc[subset_of_rows]
                results1 = results1.iloc[subset_of_rows]

            group0 = results0[metric.name].copy()
            group1 = results1[metric.name].copy()
            # Treat all NaN values as 0.
            # This is important because NaN breaks our significance testing and is common in some Ragas metrics such as Faithfulness.
            group0[np.isnan(group0)] = 0
            group1[np.isnan(group1)] = 0

            overview_label = f"{result_pair[0]} {result_pair[1]} {metric.name}"
            _, p_value, score0, score1 = print_stats_significance(
                group0, group1, overview_label, result_pair[0], result_pair[1]
            )

            result_summary_for_metric[metric.name] = {
                result_pair[0]: float(score0),
                result_pair[1]: float(score1),
                "p": p_value,
            }
        result_summary.append(result_summary_for_metric)
    return result_summary


def convert_to_dataframe(result_summary_list):
    """
    Converts the outputs of report_results_with_significance into a dataframe, mainly for use by write_df_to_workbook.
    """
    rows = []
    for result_summary_row in result_summary_list:
        for metric, models in result_summary_row.items():
            row = {"Metric": metric}
            for label, value in models.items():
                row[label] = value
            rows.append(row)

    df = pd.DataFrame(rows)

    return df


def clean_label_for_excel(s):
    """
    Converts a label into a valid worksheet name for Excel, i.e., 31 characters or less without any disallowed characters.
    """

    # Regex from Google Gemini
    pattern = r"[: \[\]\*?/\\]"
    # Note the limit of 31 characters, also for Excel
    return re.sub(pattern, "_", s[:31])


def write_df_to_workbook(df, writer, sheet_name, number_format):
    """
    Converts a label into a valid worksheet name for Excel, i.e., 31 characters or less without any disallowed characters.
    """

    sheet_name = clean_label_for_excel(sheet_name)
    df.to_excel(writer, sheet_name=sheet_name, index=False)
    worksheet = writer.sheets[sheet_name]

    for i, col in enumerate(df.columns):
        max_len = min(100, max(df[col].astype(str).apply(len).max(), len(col)) + 2)
        worksheet.set_column(i, i, max_len)
        if df[col].dtype == "float64":
            worksheet.set_column(i, i, max_len, number_format)


def write_excel(
    results,
    result_summary_all_rows,
    result_summary_rows_with_complete_reference_answers,
    output_file,
):
    # Doc string assisted by watsonx Code Assistant
    """
    This function writes the provided results and summary dataframes to an Excel file. It uses the pandas library to handle DataFrames
    and the openpyxl engine for Excel file writing. The function first sets up the Excel writer and a number format for the cells. It
    then writes the summary dataframes (if provided) and the results dataframes (from the 'results' dictionary) to the Excel file,
    using the labels from the 'results' dictionary as sheet names.

    Parameters:
    results (dict): A dictionary where keys are labels and values are pandas DataFrames containing the results.
    result_summary_all_rows (DataFrame or None): A DataFrame containing summary data for all rows.
    result_summary_rows_with_complete_reference_answers (DataFrame or None): A DataFrame containing summary data for rows with complete reference answers.
    output_file (str): The path to the output Excel file.
    """

    with pd.ExcelWriter(output_file) as writer:
        workbook = writer.book
        number_format = workbook.add_format({"num_format": "0.0000"})

        if result_summary_all_rows is not None:
            write_df_to_workbook(
                convert_to_dataframe(result_summary_all_rows),
                writer,
                "all_rows",
                number_format,
            )
        if result_summary_rows_with_complete_reference_answers is not None:
            write_df_to_workbook(
                convert_to_dataframe(
                    result_summary_rows_with_complete_reference_answers
                ),
                writer,
                "rows_with_complete_answers",
                number_format,
            )

        for label, ragas_result in results.items():
            write_df_to_workbook(ragas_result.to_pandas(), writer, label, number_format)


def run_with_retries(lambda_to_run, max_retries, delay_between_retries_seconds):
    """
    Some code to run something checking for timeout and protocol errors and then retrying.  This is important if you are using
    a hosted service such as watsonx.ai to provide the model in case the servers are having issues.
    """
    retries = 0
    while True:
        try:
            return lambda_to_run()
        except Exception as e:
            retries += 1
            if retries <= max_retries:
                LOGGER.error(
                    f"An exception of type {e.__class__.__name__} occurred.  Waiting {delay_between_retries_seconds} seconds to give the servers time to stabilize and then resuming."
                )
                # Wait 30 seconds because if there is a transient issue on the model service, then we want to give it time to clear.
                time.sleep(delay_between_retries_seconds)
            else:
                LOGGER.error(
                    f"An exception of type {e.__class__.__name__} occurred.  Maximum number of retries exceeded.  Terminating."
                )
                raise e


# Extracted prompt template as a constant
RELEVANCE_PROMPT_TEMPLATE = """
You are an expert at evaluating the intent behind responses.
Given the following Question and Candidate Answer, your task is to determine if the Candidate Answer is making a direct and genuine attempt to address or provide a response to the *specific* Question asked.

**CRITICAL RULE:** You MUST COMPLETELY IGNORE the factual accuracy or correctness of the Candidate Answer. Whether the answer is right or wrong is irrelevant for this task. Your sole focus is on its relevance and whether it appears to be an *intended, on-topic reply* to the question.

* Respond with "YES" if the Candidate Answer clearly tries to directly answer the Question, even if it's factually incorrect, incomplete, or vague.
* Respond with "NO" if the Candidate Answer is off-topic, evasive, tangential, asks another question, or discusses unrelated subjects (even if broadly similar).

---
Question: {question}
Candidate Answer: {candidate_answer}
---

Is the candidate answer attempting to answer the question?
"""


# Assisted by Google Gemini
def check_if_answer_is_attempting_to_answer_question(
    question: str, candidate_answer: str, model: LLM
) -> bool:
    """
    Constructs a prompt and calls an LLM (via LlamaIndex) to determine
    if a candidate answer is attempting to directly address the given question.

    The LLM is explicitly instructed to ignore the correctness of the answer
    and focus solely on its relevance to the question.

    Args:
        question (str): The original question asked.
        candidate_answer (str): The answer provided by an LLM or other source.
        model: The LlamaIndex LLM object to use.

    Returns:
        bool: True iff the candidate answer is attempting to answer the question.
    """

    # Use .format to fill in the values at runtime
    prompt_formatted = RELEVANCE_PROMPT_TEMPLATE.format(
        question=question, candidate_answer=candidate_answer
    )

    LOGGER.debug(f"Sending prompt to LLM:\n{prompt_formatted}")

    # Call the LLM with the constructed prompt
    response = model.complete(prompt_formatted)

    LOGGER.debug(f"LLM response:\n{response.text}")

    # Return the raw text response from the LLM
    return parse_llm_relevance_response(response.text)


# Assisted by Google Gemini
def parse_llm_relevance_response(llm_response: str) -> bool:
    """
    Parses the string response from an LLM that is expected to output "YES" or "NO"
    to indicate whether a candidate answer is attempting to answer a question.

    The function is robust to:
    - Case variations (e.g., "yes", "Yes", "YES")
    - Leading/trailing whitespace
    - Common punctuation (e.g., periods, commas)
    - Extra words before or after "YES" or "NO" (e.g., "YES, it is attempting.")

    Args:
        llm_response (str): The raw string response from the LLM.

    Returns:
        bool: True if the response indicates "YES" (attempting to answer),
              False if the response indicates "NO" (not attempting to answer)
              or if the response is unclear/malformed.
    """
    if not isinstance(llm_response, str):
        # Handle cases where input might not be a string
        LOGGER.warning(
            f"Input to parser was not a string: {type(llm_response)}. Returning False."
        )
        return False

    # 1. Convert to lowercase for case insensitivity
    processed_response = llm_response.lower()

    # 2. Remove punctuation. Create a translation table to remove all punctuation.
    # This covers cases like "YES." or "No!"
    # See https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate
    # for more about this way or removing punctuation.
    translator = str.maketrans("", "", string.punctuation)
    processed_response = processed_response.translate(translator)

    # 3. Remove extra words by splitting and checking for "yes" or "no" as primary tokens.
    # This handles "YES, it is trying" or "No, absolutely not"
    words = processed_response.split()

    # Check for "yes" in the words. We prioritize "yes" as the positive indicator.
    if "yes" in words:
        return True
    elif "no" in words:  # Explicitly check for "no" as well
        return False
    else:
        # Fallback for unexpected responses. Could be logged for debugging.
        LOGGER.warning(
            f"Warning: LLM response '{llm_response}' did not clearly contain 'yes' or 'no'. Returning False."
        )
        return False

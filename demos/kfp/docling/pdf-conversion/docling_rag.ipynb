{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install the llama stack client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "client = LlamaStackClient(base_url=\"http://llama-test-milvus-kserve-service:8321\")\n",
    "client.models.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Register your Milvus vector database with LlamaStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import Agent, AgentEventLogger, LlamaStackClient\n",
    "\n",
    "vector_db_id = \"my_demo_vector_id\"\n",
    "client = LlamaStackClient(base_url=\"http://llama-test-milvus-kserve-service:8321\")\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "# Select the first LLM and first embedding models\n",
    "model_id = next(m for m in models if m.model_type == \"llm\").identifier\n",
    "embedding_model_id = (\n",
    "    em := next(m for m in models if m.model_type == \"embedding\")\n",
    ").identifier\n",
    "embedding_dimension = em.metadata[\"embedding_dimension\"]\n",
    "\n",
    "_ = client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=embedding_model_id,\n",
    "    embedding_dimension=embedding_dimension,\n",
    "    provider_id=\"milvus\",\n",
    ")\n",
    "\n",
    "client.vector_dbs.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import and run the KubeFlow Pipeline\n",
    "Import the \"[docling_convert_pipeline_compiled.yaml](./docling_convert_pipeline_compiled.yaml)\" KubeFlow Pipeline into your pipeline server, then run the pipeline to insert your PDF documents into the vector database.\n",
    "\n",
    "When running the pipeline, you can customize the following parameters:\n",
    "\n",
    "- `base_url`: Base URL to fetch PDF files from\n",
    "- `pdf_filenames`: Comma-separated list of PDF filenames to download and convert\n",
    "- `num_workers`: Number of parallel workers\n",
    "- `vector_db_id`: Milvus vector database ID\n",
    "- `service_url`: Milvus service URL\n",
    "- `embed_model_id`: Embedding model to use\n",
    "- `max_tokens`: Maximum tokens per chunk\n",
    "- `use_gpu`: Enable/disable GPU acceleration\n",
    "\n",
    "Note: The compiled pipeline was generated by running `python docling_convert_pipeline.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompt the LLM\n",
    "Prompt the LLM with a question in relation to the documents inserted, and see it return accurate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "import uuid\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"builtin::rag/knowledge_search\",\n",
    "            \"args\": {\"vector_db_ids\": [vector_db_id]},\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"What can you tell me about the birth of word processing?\"\n",
    "print(\"prompt>\", prompt)\n",
    "\n",
    "session_id = rag_agent.create_session(session_name=f\"s{uuid.uuid4().hex}\")\n",
    "\n",
    "response = rag_agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    session_id=session_id,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for log in AgentEventLogger().log(response):\n",
    "    log.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or Query chunks from a vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = client.vector_io.query(\n",
    "    vector_db_id=vector_db_id,\n",
    "    query=\"what do you know about?\",\n",
    ")\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You've successfully inserted your PDF documents via a KubeFlow Pipeline, and queried your RAG application using Llama Stack! ðŸŽ‰ðŸ¥³"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

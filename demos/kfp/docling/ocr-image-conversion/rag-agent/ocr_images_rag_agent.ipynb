{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Llama Stack client, list available models and vector databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/vector-dbs \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models information: [Model(identifier='vllm', metadata={}, api_model_type='llm', provider_id='vllm-inference', type='model', provider_resource_id='vllm', model_type='llm'), Model(identifier='ibm-granite/granite-embedding-125m-english', metadata={'embedding_dimension': 768.0}, api_model_type='embedding', provider_id='sentence-transformers', type='model', provider_resource_id='ibm-granite/granite-embedding-125m-english', model_type='embedding')]\n",
      "\n",
      "Identifier for Inference model in usage: vllm\n",
      "\n",
      "=== Available Vector Databases ===\n",
      "- ID: my_demo_image_ocr_vector_id\n",
      "  Provider: milvus\n",
      "  Embedding Model: ibm-granite/granite-embedding-125m-english\n",
      "\n",
      "- ID: demo_db\n",
      "  Provider: milvus\n",
      "  Embedding Model: ibm-granite/granite-embedding-125m-english\n",
      "\n",
      "- ID: ocr-vector-db\n",
      "  Provider: milvus\n",
      "  Embedding Model: ibm-granite/granite-embedding-125m-english\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "client = LlamaStackClient(base_url=\"http://lsd-llama-milvus-service:8321\")\n",
    "\n",
    "models = client.models.list()\n",
    "print(f\"Models information: {models}\\n\")\n",
    "\n",
    "inference_llm = next((model.identifier for model in models if model.model_type == 'llm'), None)\n",
    "print(f\"Identifier for Inference model in usage: {inference_llm}\\n\")\n",
    "\n",
    "# Check what vector databases exist\n",
    "print(\"=== Available Vector Databases ===\")\n",
    "vector_dbs = client.vector_dbs.list()\n",
    "if vector_dbs:\n",
    "    for vdb in vector_dbs:\n",
    "        print(f\"- ID: {vdb.identifier}\")\n",
    "        print(f\"  Provider: {vdb.provider_id}\")\n",
    "        print(f\"  Embedding Model: {vdb.embedding_model}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No vector databases found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create RAG Agent and prompt the LLM\n",
    "Prompt the LLM with questions in relation to the documents inserted, and see it return accurate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/tools?toolgroup_id=builtin%3A%3Arag%2Fknowledge_search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/a56ac9ff-2d7b-4beb-ae47-a2e5b08b6ae1/session \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/a56ac9ff-2d7b-4beb-ae47-a2e5b08b6ae1/session/960bb726-d8df-40e6-9c9a-c8244e04268d/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> List RAG key market use cases\n",
      "\u001b[33minference> \u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'RAG key market use cases'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\n- Knowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\n- Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\n- Recommendation Systems: Enhancing recommendations by providing relevant context.\\n- Customer Service: Improving support accuracy with access to current product information.\\n- Personal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\n- Multi-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\n- Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\\n- General Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is also a significant driver for RAG adoption; allowing for tailored ad copy and product recommendations .\\nMetadata: {'file_name': 'RAG_key_market_usecases', 'document_id': '7fc8ce0f-82ff-4ef2-9dd6-afac8bed54fc'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\n- Knowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\n- Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\n- Recommendation Systems: Enhancing recommendations by providing relevant context.\\n- Customer Service: Improving support accuracy with access to current product information.\\n- Personal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\n- Multi-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\n- Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\\n- General Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is also a significant driver for RAG adoption; allowing for tailored ad copy and product recommendations .\\nMetadata: {'file_name': 'RAG_key_market_usecases', 'document_id': 'e2ea04ff-2833-4ce2-8248-8ae751fdeed7'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\n- Knowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\n- Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\n- Recommendation Systems: Enhancing recommendations by providing relevant context.\\n- Customer Service: Improving support accuracy with access to current product information.\\n- Personal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\n- Multi-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\n- Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\\n- General Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is also a significant driver for RAG adoption; allowing for tailored ad copy and product recommendations .\\nMetadata: {'file_name': 'RAG_key_market_usecases', 'document_id': 'b94fb058-bff6-4975-b6de-c6551ce52e7c'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: Ingeslion Flow\\nDocument Upload (vla\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\nQuery Select Wodel\\nMetadata: {'file_name': 'RAG_flow_diagram', 'document_id': 'ad87ea2b-e78b-41d1-bbd8-78ab16c8c792'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: Ingeslion Flow\\nDocument Upload (vla\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\nQuery Select Wodel\\nMetadata: {'file_name': 'RAG_flow_diagram', 'document_id': '77417efb-07e1-4d82-bfa4-f6d739ed2f09'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"RAG key market use cases\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mR\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m key\u001b[0m\u001b[33m market\u001b[0m\u001b[33m use\u001b[0m\u001b[33m cases\u001b[0m\u001b[33m include\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Knowledge\u001b[0m\u001b[33m Question\u001b[0m\u001b[33m Answer\u001b[0m\u001b[33ming\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Providing\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m answers\u001b[0m\u001b[33m in\u001b[0m\u001b[33m customer\u001b[0m\u001b[33m service\u001b[0m\u001b[33m product\u001b[0m\u001b[33m manuals\u001b[0m\u001b[33m or\u001b[0m\u001b[33m FAQs\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Code\u001b[0m\u001b[33m Generation\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Retrie\u001b[0m\u001b[33mving\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m code\u001b[0m\u001b[33m snippets\u001b[0m\u001b[33m and\u001b[0m\u001b[33m documentation\u001b[0m\u001b[33m to\u001b[0m\u001b[33m assist\u001b[0m\u001b[33m in\u001b[0m\u001b[33m code\u001b[0m\u001b[33m creation\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Recommendation\u001b[0m\u001b[33m Systems\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Enh\u001b[0m\u001b[33mancing\u001b[0m\u001b[33m recommendations\u001b[0m\u001b[33m by\u001b[0m\u001b[33m providing\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m context\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Customer\u001b[0m\u001b[33m Service\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Impro\u001b[0m\u001b[33mving\u001b[0m\u001b[33m support\u001b[0m\u001b[33m accuracy\u001b[0m\u001b[33m with\u001b[0m\u001b[33m access\u001b[0m\u001b[33m to\u001b[0m\u001b[33m current\u001b[0m\u001b[33m product\u001b[0m\u001b[33m information\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Personal\u001b[0m\u001b[33m Assist\u001b[0m\u001b[33mants\u001b[0m\u001b[33m:\u001b[0m\u001b[33m En\u001b[0m\u001b[33mabling\u001b[0m\u001b[33m more\u001b[0m\u001b[33m comprehensive\u001b[0m\u001b[33m and\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m information\u001b[0m\u001b[33m from\u001b[0m\u001b[33m Al\u001b[0m\u001b[33m assistants\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Multi\u001b[0m\u001b[33m-hop\u001b[0m\u001b[33m Question\u001b[0m\u001b[33m Answer\u001b[0m\u001b[33ming\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Handling\u001b[0m\u001b[33m complex\u001b[0m\u001b[33m;\u001b[0m\u001b[33m multi\u001b[0m\u001b[33m-step\u001b[0m\u001b[33m questions\u001b[0m\u001b[33m through\u001b[0m\u001b[33m iterative\u001b[0m\u001b[33m retrieval\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Legal\u001b[0m\u001b[33m Applications\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Retrie\u001b[0m\u001b[33mving\u001b[0m\u001b[33m legal\u001b[0m\u001b[33m documents\u001b[0m\u001b[33m and\u001b[0m\u001b[33m case\u001b[0m\u001b[33m law\u001b[0m\u001b[33m for\u001b[0m\u001b[33m reliable\u001b[0m\u001b[33m legal\u001b[0m\u001b[33m opinions\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m General\u001b[0m\u001b[33m Task\u001b[0m\u001b[33m Assistance\u001b[0m\u001b[33m:\u001b[0m\u001b[33m A\u001b[0m\u001b[33miding\u001b[0m\u001b[33m users\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m tasks\u001b[0m\u001b[33m requiring\u001b[0m\u001b[33m information\u001b[0m\u001b[33m access\u001b[0m\u001b[33m and\u001b[0m\u001b[33m decision\u001b[0m\u001b[33m-making\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThe\u001b[0m\u001b[33m rising\u001b[0m\u001b[33m demand\u001b[0m\u001b[33m for\u001b[0m\u001b[33m hyper\u001b[0m\u001b[33m-person\u001b[0m\u001b[33mal\u001b[0m\u001b[33mized\u001b[0m\u001b[33m content\u001b[0m\u001b[33m in\u001b[0m\u001b[33m areas\u001b[0m\u001b[33m like\u001b[0m\u001b[33m marketing\u001b[0m\u001b[33m and\u001b[0m\u001b[33m e\u001b[0m\u001b[33m-commerce\u001b[0m\u001b[33m is\u001b[0m\u001b[33m also\u001b[0m\u001b[33m a\u001b[0m\u001b[33m significant\u001b[0m\u001b[33m driver\u001b[0m\u001b[33m for\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m adoption\u001b[0m\u001b[33m,\u001b[0m\u001b[33m allowing\u001b[0m\u001b[33m for\u001b[0m\u001b[33m tailored\u001b[0m\u001b[33m ad\u001b[0m\u001b[33m copy\u001b[0m\u001b[33m and\u001b[0m\u001b[33m product\u001b[0m\u001b[33m recommendations\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/a56ac9ff-2d7b-4beb-ae47-a2e5b08b6ae1/session/960bb726-d8df-40e6-9c9a-c8244e04268d/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> Describe the sequence of steps of the Ingestion Flow\n",
      "\u001b[33minference> \u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'RAG Ingestion Flow sequence of steps'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\n- Knowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\n- Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\n- Recommendation Systems: Enhancing recommendations by providing relevant context.\\n- Customer Service: Improving support accuracy with access to current product information.\\n- Personal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\n- Multi-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\n- Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\\n- General Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is also a significant driver for RAG adoption; allowing for tailored ad copy and product recommendations .\\nMetadata: {'file_name': 'RAG_key_market_usecases', 'document_id': '7fc8ce0f-82ff-4ef2-9dd6-afac8bed54fc'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\n- Knowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\n- Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\n- Recommendation Systems: Enhancing recommendations by providing relevant context.\\n- Customer Service: Improving support accuracy with access to current product information.\\n- Personal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\n- Multi-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\n- Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\\n- General Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is also a significant driver for RAG adoption; allowing for tailored ad copy and product recommendations .\\nMetadata: {'file_name': 'RAG_key_market_usecases', 'document_id': 'e2ea04ff-2833-4ce2-8248-8ae751fdeed7'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\n- Knowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\n- Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\n- Recommendation Systems: Enhancing recommendations by providing relevant context.\\n- Customer Service: Improving support accuracy with access to current product information.\\n- Personal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\n- Multi-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\n- Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\\n- General Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is also a significant driver for RAG adoption; allowing for tailored ad copy and product recommendations .\\nMetadata: {'file_name': 'RAG_key_market_usecases', 'document_id': 'b94fb058-bff6-4975-b6de-c6551ce52e7c'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: Ingeslion Flow\\nDocument Upload (vla\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\nQuery Select Wodel\\nMetadata: {'file_name': 'RAG_flow_diagram', 'document_id': 'ad87ea2b-e78b-41d1-bbd8-78ab16c8c792'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: Ingeslion Flow\\nDocument Upload (vla\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\nQuery Select Wodel\\nMetadata: {'file_name': 'RAG_flow_diagram', 'document_id': '77417efb-07e1-4d82-bfa4-f6d739ed2f09'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"RAG Ingestion Flow sequence of steps\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mThe\u001b[0m\u001b[33m sequence\u001b[0m\u001b[33m of\u001b[0m\u001b[33m steps\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m In\u001b[0m\u001b[33mgest\u001b[0m\u001b[33mion\u001b[0m\u001b[33m Flow\u001b[0m\u001b[33m of\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m is\u001b[0m\u001b[33m not\u001b[0m\u001b[33m explicitly\u001b[0m\u001b[33m stated\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m.\u001b[0m\u001b[33m However\u001b[0m\u001b[33m,\u001b[0m\u001b[33m based\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m context\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m it\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m inferred\u001b[0m\u001b[33m that\u001b[0m\u001b[33m the\u001b[0m\u001b[33m In\u001b[0m\u001b[33mgest\u001b[0m\u001b[33mion\u001b[0m\u001b[33m Flow\u001b[0m\u001b[33m involves\u001b[0m\u001b[33m the\u001b[0m\u001b[33m following\u001b[0m\u001b[33m steps\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Document\u001b[0m\u001b[33m Upload\u001b[0m\u001b[33m:\u001b[0m\u001b[33m The\u001b[0m\u001b[33m process\u001b[0m\u001b[33m of\u001b[0m\u001b[33m uploading\u001b[0m\u001b[33m documents\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m system\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Doc\u001b[0m\u001b[33mling\u001b[0m\u001b[33m (\u001b[0m\u001b[33mChunk\u001b[0m\u001b[33ming\u001b[0m\u001b[33m):\u001b[0m\u001b[33m The\u001b[0m\u001b[33m process\u001b[0m\u001b[33m of\u001b[0m\u001b[33m breaking\u001b[0m\u001b[33m down\u001b[0m\u001b[33m the\u001b[0m\u001b[33m uploaded\u001b[0m\u001b[33m documents\u001b[0m\u001b[33m into\u001b[0m\u001b[33m smaller\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m or\u001b[0m\u001b[33m segments\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Embed\u001b[0m\u001b[33mding\u001b[0m\u001b[33m (\u001b[0m\u001b[33mGran\u001b[0m\u001b[33mite\u001b[0m\u001b[33m):\u001b[0m\u001b[33m The\u001b[0m\u001b[33m process\u001b[0m\u001b[33m of\u001b[0m\u001b[33m embedding\u001b[0m\u001b[33m the\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m into\u001b[0m\u001b[33m a\u001b[0m\u001b[33m larger\u001b[0m\u001b[33m context\u001b[0m\u001b[33m or\u001b[0m\u001b[33m knowledge\u001b[0m\u001b[33m graph\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Embed\u001b[0m\u001b[33mding\u001b[0m\u001b[33m via\u001b[0m\u001b[33m v\u001b[0m\u001b[33mLL\u001b[0m\u001b[33mM\u001b[0m\u001b[33m:\u001b[0m\u001b[33m The\u001b[0m\u001b[33m process\u001b[0m\u001b[33m of\u001b[0m\u001b[33m using\u001b[0m\u001b[33m a\u001b[0m\u001b[33m large\u001b[0m\u001b[33m language\u001b[0m\u001b[33m model\u001b[0m\u001b[33m (\u001b[0m\u001b[33mv\u001b[0m\u001b[33mLL\u001b[0m\u001b[33mM\u001b[0m\u001b[33m)\u001b[0m\u001b[33m to\u001b[0m\u001b[33m further\u001b[0m\u001b[33m embed\u001b[0m\u001b[33m the\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m into\u001b[0m\u001b[33m a\u001b[0m\u001b[33m more\u001b[0m\u001b[33m nuanced\u001b[0m\u001b[33m and\u001b[0m\u001b[33m contextual\u001b[0m\u001b[33mized\u001b[0m\u001b[33m representation\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m (\u001b[0m\u001b[33mY\u001b[0m\u001b[33mector\u001b[0m\u001b[33m DB\u001b[0m\u001b[33m):\u001b[0m\u001b[33m The\u001b[0m\u001b[33m process\u001b[0m\u001b[33m of\u001b[0m\u001b[33m storing\u001b[0m\u001b[33m the\u001b[0m\u001b[33m embedded\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m in\u001b[0m\u001b[33m a\u001b[0m\u001b[33m database\u001b[0m\u001b[33m (\u001b[0m\u001b[33mY\u001b[0m\u001b[33mector\u001b[0m\u001b[33m DB\u001b[0m\u001b[33m)\u001b[0m\u001b[33m for\u001b[0m\u001b[33m later\u001b[0m\u001b[33m retrieval\u001b[0m\u001b[33m and\u001b[0m\u001b[33m use\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m6\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Top\u001b[0m\u001b[33m-K\u001b[0m\u001b[33m Ch\u001b[0m\u001b[33munks\u001b[0m\u001b[33m:\u001b[0m\u001b[33m The\u001b[0m\u001b[33m process\u001b[0m\u001b[33m of\u001b[0m\u001b[33m selecting\u001b[0m\u001b[33m the\u001b[0m\u001b[33m top\u001b[0m\u001b[33m-k\u001b[0m\u001b[33m most\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m or\u001b[0m\u001b[33m useful\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m from\u001b[0m\u001b[33m the\u001b[0m\u001b[33m database\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m7\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Prompt\u001b[0m\u001b[33m Constructor\u001b[0m\u001b[33m:\u001b[0m\u001b[33m The\u001b[0m\u001b[33m process\u001b[0m\u001b[33m of\u001b[0m\u001b[33m constructing\u001b[0m\u001b[33m a\u001b[0m\u001b[33m prompt\u001b[0m\u001b[33m or\u001b[0m\u001b[33m query\u001b[0m\u001b[33m that\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m used\u001b[0m\u001b[33m to\u001b[0m\u001b[33m retrieve\u001b[0m\u001b[33m the\u001b[0m\u001b[33m selected\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m8\u001b[0m\u001b[33m.\u001b[0m\u001b[33m (\u001b[0m\u001b[33mll\u001b[0m\u001b[33mama\u001b[0m\u001b[33m-stack\u001b[0m\u001b[33m):\u001b[0m\u001b[33m The\u001b[0m\u001b[33m process\u001b[0m\u001b[33m of\u001b[0m\u001b[33m using\u001b[0m\u001b[33m a\u001b[0m\u001b[33m stack\u001b[0m\u001b[33m-based\u001b[0m\u001b[33m architecture\u001b[0m\u001b[33m to\u001b[0m\u001b[33m process\u001b[0m\u001b[33m the\u001b[0m\u001b[33m prompt\u001b[0m\u001b[33m and\u001b[0m\u001b[33m retrieve\u001b[0m\u001b[33m the\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m9\u001b[0m\u001b[33m.\u001b[0m\u001b[33m User\u001b[0m\u001b[33m Interface\u001b[0m\u001b[33m:\u001b[0m\u001b[33m The\u001b[0m\u001b[33m process\u001b[0m\u001b[33m of\u001b[0m\u001b[33m presenting\u001b[0m\u001b[33m the\u001b[0m\u001b[33m retrieved\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m user\u001b[0m\u001b[33m in\u001b[0m\u001b[33m a\u001b[0m\u001b[33m user\u001b[0m\u001b[33m-friendly\u001b[0m\u001b[33m interface\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m10\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Query\u001b[0m\u001b[33m:\u001b[0m\u001b[33m The\u001b[0m\u001b[33m process\u001b[0m\u001b[33m of\u001b[0m\u001b[33m sending\u001b[0m\u001b[33m a\u001b[0m\u001b[33m query\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m system\u001b[0m\u001b[33m to\u001b[0m\u001b[33m retrieve\u001b[0m\u001b[33m the\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m11\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Query\u001b[0m\u001b[33m Embed\u001b[0m\u001b[33mding\u001b[0m\u001b[33m:\u001b[0m\u001b[33m The\u001b[0m\u001b[33m process\u001b[0m\u001b[33m of\u001b[0m\u001b[33m embedding\u001b[0m\u001b[33m the\u001b[0m\u001b[33m query\u001b[0m\u001b[33m into\u001b[0m\u001b[33m a\u001b[0m\u001b[33m vector\u001b[0m\u001b[33m space\u001b[0m\u001b[33m to\u001b[0m\u001b[33m facilitate\u001b[0m\u001b[33m retrieval\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m12\u001b[0m\u001b[33m.\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m In\u001b[0m\u001b[33mference\u001b[0m\u001b[33m:\u001b[0m\u001b[33m The\u001b[0m\u001b[33m process\u001b[0m\u001b[33m of\u001b[0m\u001b[33m using\u001b[0m\u001b[33m the\u001b[0m\u001b[33m large\u001b[0m\u001b[33m language\u001b[0m\u001b[33m model\u001b[0m\u001b[33m to\u001b[0m\u001b[33m infer\u001b[0m\u001b[33m the\u001b[0m\u001b[33m relevance\u001b[0m\u001b[33m and\u001b[0m\u001b[33m context\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m query\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m13\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Response\u001b[0m\u001b[33m:\u001b[0m\u001b[33m The\u001b[0m\u001b[33m process\u001b[0m\u001b[33m of\u001b[0m\u001b[33m generating\u001b[0m\u001b[33m a\u001b[0m\u001b[33m response\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m query\u001b[0m\u001b[33m based\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m retrieved\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m14\u001b[0m\u001b[33m.\u001b[0m\u001b[33m J\u001b[0m\u001b[33mupyter\u001b[0m\u001b[33m Note\u001b[0m\u001b[33mbooks\u001b[0m\u001b[33m:\u001b[0m\u001b[33m The\u001b[0m\u001b[33m process\u001b[0m\u001b[33m of\u001b[0m\u001b[33m using\u001b[0m\u001b[33m J\u001b[0m\u001b[33mupyter\u001b[0m\u001b[33m Note\u001b[0m\u001b[33mbooks\u001b[0m\u001b[33m to\u001b[0m\u001b[33m run\u001b[0m\u001b[33m and\u001b[0m\u001b[33m debug\u001b[0m\u001b[33m the\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m system\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m15\u001b[0m\u001b[33m.\u001b[0m\u001b[33m (\u001b[0m\u001b[33mRun\u001b[0m\u001b[33m Debug\u001b[0m\u001b[33m):\u001b[0m\u001b[33m The\u001b[0m\u001b[33m process\u001b[0m\u001b[33m of\u001b[0m\u001b[33m running\u001b[0m\u001b[33m and\u001b[0m\u001b[33m debugging\u001b[0m\u001b[33m the\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m system\u001b[0m\u001b[33m to\u001b[0m\u001b[33m ensure\u001b[0m\u001b[33m that\u001b[0m\u001b[33m it\u001b[0m\u001b[33m is\u001b[0m\u001b[33m functioning\u001b[0m\u001b[33m correctly\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mNote\u001b[0m\u001b[33m that\u001b[0m\u001b[33m this\u001b[0m\u001b[33m is\u001b[0m\u001b[33m an\u001b[0m\u001b[33m inferred\u001b[0m\u001b[33m sequence\u001b[0m\u001b[33m of\u001b[0m\u001b[33m steps\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m the\u001b[0m\u001b[33m actual\u001b[0m\u001b[33m sequence\u001b[0m\u001b[33m of\u001b[0m\u001b[33m steps\u001b[0m\u001b[33m may\u001b[0m\u001b[33m vary\u001b[0m\u001b[33m depending\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m implementation\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m system\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/a56ac9ff-2d7b-4beb-ae47-a2e5b08b6ae1/session/960bb726-d8df-40e6-9c9a-c8244e04268d/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> What is the state of Irish eceonomy at 2025?\n",
      "\u001b[33minference> \u001b[0m\u001b[33mI\u001b[0m\u001b[33m don\u001b[0m\u001b[33m’t\u001b[0m\u001b[33m know\u001b[0m\u001b[33m what\u001b[0m\u001b[33m the\u001b[0m\u001b[33m state\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m Irish\u001b[0m\u001b[33m economy\u001b[0m\u001b[33m is\u001b[0m\u001b[33m in\u001b[0m\u001b[33m \u001b[0m\u001b[33m202\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/agents/a56ac9ff-2d7b-4beb-ae47-a2e5b08b6ae1/session/960bb726-d8df-40e6-9c9a-c8244e04268d \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "import uuid\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=\"vllm\",\n",
    "    instructions=\"You are a helpful assistant. Answer the user's question based only on the provided search results. Respond with 'I don’t know' if the information is outside of the scope of your knowledge and not present in the search results.\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"builtin::rag/knowledge_search\",\n",
    "            \"args\": {\"vector_db_ids\": [\"ocr-vector-db\"]},\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "user_prompts = [\n",
    "    \"List RAG key market use cases\",\n",
    "    \"Describe the sequence of steps of the Ingestion Flow\",\n",
    "    \"What is the state of Irish economy at 2025?\", # Dummy question to show that Agent is allowed to respond only if question relates to the uploaded data\n",
    "]\n",
    "\n",
    "session_id = rag_agent.create_session(session_name=f\"s{uuid.uuid4().hex}\")\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(\"prompt>\", prompt)\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "        stream=True,\n",
    "    )\n",
    "    for log in AgentEventLogger().log(response):\n",
    "        log.print()\n",
    "\n",
    "session_response = client.agents.session.retrieve(\n",
    "    session_id=session_id,\n",
    "    agent_id=rag_agent.agent_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparation for evaluating RAG models using [RAGAS](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/?h=metrics)\n",
    "\n",
    "- We will use two key metrics to show the performance of the RAG server:\n",
    "    1. [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/) - measures how factually consistent a response is with the retrieved context. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "    2. [Response Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/) - metric measures how relevant a response is to the user input. Higher scores indicate better alignment with the user input, while lower scores are given if the response is incomplete or includes redundant information.\n",
    "\n",
    " - Create .env.dev file and paste there your API Key from [Groq Cloud](https://console.groq.com/home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "with open(\".env.dev\", \"w\") as f:\n",
    "    f.write('GROQ_API_KEY=PASTE_YOUR_GROQ_API_KEY')\n",
    "\n",
    "# load env variable\n",
    "load_dotenv(dotenv_path=\".env.dev\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from llama_stack_client.types.agents import Turn\n",
    "\n",
    "# Compile regex pattern once for better performance\n",
    "CONTENT_PATTERN = re.compile(r\"Content:\\s*(.*?)(?=\\nMetadata:|$)\", re.DOTALL)\n",
    "\n",
    "# This function extracts the search results for the trace of each query\n",
    "def extract_retrieved_contexts(turn_object: Turn) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts retrieved contexts from LlamaStack tool execution responses.\n",
    "    \n",
    "    Args:\n",
    "        turn_object: A Turn object from LlamaStack containing steps with tool responses\n",
    "        \n",
    "    Returns:\n",
    "        List of retrieved context strings for Ragas evaluation\n",
    "    \"\"\"\n",
    "    retrieved_context = []\n",
    "\n",
    "    # Filter tool execution steps first to reduce iterations\n",
    "    tool_steps = [step for step in turn_object.steps if step.step_type == \"tool_execution\"]\n",
    "    \n",
    "    for step in tool_steps:\n",
    "        for response in step.tool_responses:\n",
    "            if not response.content or not isinstance(response.content, list):\n",
    "                continue\n",
    "                \n",
    "            # Process all valid text items at once\n",
    "            text_items = [\n",
    "                item.text for item in response.content \n",
    "                if (hasattr(item, \"text\") and hasattr(item, \"type\") and \n",
    "                    item.type == \"text\" and item.text and \n",
    "                    item.text.startswith(\"Result \") and \"Content:\" in item.text)\n",
    "            ]\n",
    "            \n",
    "            # Extract content from all valid texts\n",
    "            for text in text_items:\n",
    "                match = CONTENT_PATTERN.search(text)\n",
    "                if match:\n",
    "                    retrieved_context.append(match.group(1).strip())\n",
    "\n",
    "    return retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>List RAG key market use cases</td>\n",
       "      <td>[Market Use Cases Key\\nRAG is being adopted ac...</td>\n",
       "      <td>RAG key market use cases include:\\n\\n* Knowled...</td>\n",
       "      <td>\\n1. Knowledge Question Answering: Providing a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Describe the sequence of steps of the Ingestio...</td>\n",
       "      <td>[Market Use Cases Key\\nRAG is being adopted ac...</td>\n",
       "      <td>The sequence of steps in the Ingestion Flow of...</td>\n",
       "      <td>\\nIngestion Flow:\\n1. Document Upload (via UI/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                      List RAG key market use cases   \n",
       "1  Describe the sequence of steps of the Ingestio...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [Market Use Cases Key\\nRAG is being adopted ac...   \n",
       "1  [Market Use Cases Key\\nRAG is being adopted ac...   \n",
       "\n",
       "                                            response  \\\n",
       "0  RAG key market use cases include:\\n\\n* Knowled...   \n",
       "1  The sequence of steps in the Ingestion Flow of...   \n",
       "\n",
       "                                           reference  \n",
       "0  \\n1. Knowledge Question Answering: Providing a...  \n",
       "1  \\nIngestion Flow:\\n1. Document Upload (via UI/...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import EvaluationDataset\n",
    "\n",
    "samples = []\n",
    "\n",
    "references = ['''\n",
    "1. Knowledge Question Answering: Providing accurate answers in customer service, product manuals, or FAQs.\n",
    "2. Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\n",
    "3. Recommendation Systems: Enhancing recommendations by providing relevant context.\n",
    "4. Customer Service: Improving support accuracy with access to current product information.\n",
    "5. Personal Assistants: Enabling more comprehensive and accurate information from AI assistants.\n",
    "6. Multi-hop Question Answering: Handling complex, multi-step questions through iterative retrieval.\n",
    "7. Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\n",
    "8. General Task Assistance: Aiding users in various tasks requiring information access and decision-making.''', \n",
    "'''\n",
    "Ingestion Flow:\n",
    "1. Document Upload (via UI/API)\n",
    "2. Docling (Chunking + Metadata)\n",
    "3. Embedding (Granite Embedding via vLLM)\n",
    "4. Milvus (Vector DB)\n",
    "'''\n",
    "]\n",
    "\n",
    "# Constructing a Ragas EvaluationDataset\n",
    "for i, turn in enumerate(session_response.turns[:2]):\n",
    "    samples.append(\n",
    "        {\n",
    "            \"user_input\": turn.input_messages[0].content,\n",
    "            \"response\": turn.output_message.content,\n",
    "            \"reference\": references[i],\n",
    "            \"retrieved_contexts\": extract_retrieved_contexts(turn),\n",
    "        }\n",
    "    )\n",
    "\n",
    "ragas_eval_dataset = EvaluationDataset.from_list(samples)\n",
    "ragas_eval_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prerequisites for RAG evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: ibm-granite/granite-embedding-125m-english\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved contexts for the first prompt: ['Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\n- Knowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\n- Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\n- Recommendation Systems: Enhancing recommendations by providing relevant context.\\n- Customer Service: Improving support accuracy with access to current product information.\\n- Personal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\n- Multi-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\n- Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\\n- General Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is also a significant driver for RAG adoption; allowing for tailored ad copy and product recommendations .', 'Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\n- Knowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\n- Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\n- Recommendation Systems: Enhancing recommendations by providing relevant context.\\n- Customer Service: Improving support accuracy with access to current product information.\\n- Personal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\n- Multi-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\n- Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\\n- General Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is also a significant driver for RAG adoption; allowing for tailored ad copy and product recommendations .', 'Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\n- Knowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\n- Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\n- Recommendation Systems: Enhancing recommendations by providing relevant context.\\n- Customer Service: Improving support accuracy with access to current product information.\\n- Personal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\n- Multi-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\n- Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\\n- General Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is also a significant driver for RAG adoption; allowing for tailored ad copy and product recommendations .', 'Ingeslion Flow\\nDocument Upload (vla\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\nQuery Select Wodel', 'Ingeslion Flow\\nDocument Upload (vla\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\nQuery Select Wodel']\n",
      "\n",
      "Retrieved contexts for the second prompt: ['Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\n- Knowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\n- Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\n- Recommendation Systems: Enhancing recommendations by providing relevant context.\\n- Customer Service: Improving support accuracy with access to current product information.\\n- Personal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\n- Multi-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\n- Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\\n- General Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is also a significant driver for RAG adoption; allowing for tailored ad copy and product recommendations .', 'Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\n- Knowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\n- Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\n- Recommendation Systems: Enhancing recommendations by providing relevant context.\\n- Customer Service: Improving support accuracy with access to current product information.\\n- Personal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\n- Multi-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\n- Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\\n- General Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is also a significant driver for RAG adoption; allowing for tailored ad copy and product recommendations .', 'Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\n- Knowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\n- Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\n- Recommendation Systems: Enhancing recommendations by providing relevant context.\\n- Customer Service: Improving support accuracy with access to current product information.\\n- Personal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\n- Multi-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\n- Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\\n- General Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is also a significant driver for RAG adoption; allowing for tailored ad copy and product recommendations .', 'Ingeslion Flow\\nDocument Upload (vla\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\nQuery Select Wodel', 'Ingeslion Flow\\nDocument Upload (vla\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\nQuery Select Wodel']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import (\n",
    "    Faithfulness, \n",
    "    ResponseRelevancy,\n",
    ") \n",
    "from ragas.dataset_schema import SingleTurnSample \n",
    "from langchain_groq import ChatGroq\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Wrap the Groq LLM for use with Ragas\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "# Using HuggingFace embeddings as a free alternative\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"ibm-granite/granite-embedding-125m-english\"\n",
    ")\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings_model)\n",
    "\n",
    "\n",
    "# references for both prompts\n",
    "reference_for_first_prompt = samples[0][\"reference\"]\n",
    "reference_for_second_prompt = samples[1][\"reference\"]\n",
    "\n",
    "# inputs for both prompts\n",
    "user_input_for_first_prompt = samples[0][\"user_input\"]\n",
    "user_input_for_second_prompt = samples[1][\"user_input\"]\n",
    "\n",
    "# responses for both prompts\n",
    "response_for_first_prompt = samples[0][\"response\"]\n",
    "response_for_second_prompt = samples[1][\"response\"]\n",
    "\n",
    "# reference lists for both prompts\n",
    "reference_list_for_first_prompt = [line.strip() for line in reference_for_first_prompt.strip().split('\\n')]\n",
    "reference_list_for_second_prompt = [line.strip() for line in reference_for_second_prompt.strip().split('\\n')]\n",
    "\n",
    "# Retrieved contexts for both prompts\n",
    "retrieved_contexts_for_first_prompt = samples[0][\"retrieved_contexts\"]\n",
    "retrieved_contexts_for_second_prompt = samples[1][\"retrieved_contexts\"]\n",
    "\n",
    "print(f\"Retrieved contexts for the first prompt: {retrieved_contexts_for_first_prompt}\\n\")\n",
    "print(f\"Retrieved contexts for the second prompt: {retrieved_contexts_for_second_prompt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Faithfulness Score for both prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness score for prompt 'List RAG key market use cases': 1.0\n"
     ]
    }
   ],
   "source": [
    "first_prompt_turn = SingleTurnSample(\n",
    "        user_input=user_input_for_first_prompt,\n",
    "        response=response_for_first_prompt,\n",
    "        retrieved_contexts=retrieved_contexts_for_first_prompt,\n",
    "    )\n",
    "faithfulness_scorer = Faithfulness(llm=evaluator_llm)\n",
    "faithfulness_score_for_first_prompt = await faithfulness_scorer.single_turn_ascore(first_prompt_turn)\n",
    "print(f\"Faithfulness score for prompt '{user_prompts[0]}': {faithfulness_score_for_first_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness score for prompt 'Describe the sequence of steps of the Ingestion Flow': 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "second_prompt_turn = SingleTurnSample(\n",
    "        user_input=user_input_for_second_prompt,\n",
    "        response=response_for_second_prompt,\n",
    "        retrieved_contexts=retrieved_contexts_for_second_prompt,\n",
    "    )\n",
    "faithfulness_score_for_second_prompt = await faithfulness_scorer.single_turn_ascore(second_prompt_turn)\n",
    "print(f\"Faithfulness score for prompt '{user_prompts[1]}': {faithfulness_score_for_second_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Response Relevancy for both prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Relevancy score for prompt 'List RAG key market use cases': 0.9634198534980678\n"
     ]
    }
   ],
   "source": [
    "first_prompt_turn = SingleTurnSample(\n",
    "        user_input=user_input_for_first_prompt,\n",
    "        response=response_for_first_prompt,\n",
    "        retrieved_contexts=retrieved_contexts_for_first_prompt,\n",
    "    )\n",
    "response_relevancy_scorer = ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)\n",
    "response_relevancy_score_for_first_prompt = await response_relevancy_scorer.single_turn_ascore(first_prompt_turn)\n",
    "print(f\"Response Relevancy score for prompt '{user_prompts[0]}': {response_relevancy_score_for_first_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Relevancy score for prompt 'Describe the sequence of steps of the Ingestion Flow': 0.9282959928630402\n"
     ]
    }
   ],
   "source": [
    "second_prompt_turn = SingleTurnSample(\n",
    "        user_input=user_input_for_second_prompt,\n",
    "        response=response_for_second_prompt,\n",
    "        retrieved_contexts=retrieved_contexts_for_second_prompt,\n",
    "    )\n",
    "response_relevancy_score_for_second_prompt = await response_relevancy_scorer.single_turn_ascore(second_prompt_turn)\n",
    "print(f\"Response Relevancy score for prompt '{user_prompts[1]}': {response_relevancy_score_for_second_prompt}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
